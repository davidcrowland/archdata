{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Connecting to postgresql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little introduction to the concepts of Postgresql and Pandas interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To retrieve data from the postgresql database, I use [Pandas](http://pandas.pydata.org/pandas-docs/stable/index.html), which basically has Postgresql interaction built in and returns *dataframe* objects that can be easily displayed, filtered, plotted and so on. \n",
    "\n",
    "Interaction with the data happens via **queries** that are used to retrieve a subset of the data scattered throughout the database. These queries can have simple to complex forms and use their own (postgresql specific) language, which is quite easy to understand. The beauty about these queries is that they allow filtering on the flow, sorting, and - maybe most importantly - the merging of different tables matching certain criteria. Common tasks are for example the retrieval of data within a user specified range from a specific table or the data that has a common ID in two tables. Examples will be given below and in the next notebooks.\n",
    "\n",
    "Authentication happens in two ways: Every computer has to have a password file saved which will be automatically accessed by postgres and will be used to log in to a specific database as a specific user. Additionally, the basic parameters are set in subfunctions (saved under `/database_helpers`).\n",
    "\n",
    "This computer here has read-only access to the database, but your office computer has administrator privilege access because it needs also to change / edit data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every interaction starts with the basic import of libraries and functions ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the following two lines indicate that external functions are auto-reloaded as soon as they change. \n",
    "# That's a nice trick!\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Print statements \n",
    "from __future__ import print_function # Python 2.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded analysis helpers: General\n"
     ]
    }
   ],
   "source": [
    "# General stuff:\n",
    "import sys\n",
    "import argparse\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import math\n",
    "import psycopg2\n",
    "import cPickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "# Plotting:\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "\n",
    "# External functions from subfolder /database_helpers. \n",
    "# as soon as you change something in there and press save, it will auto reload on next execution.\n",
    "from database_helpers.psql_start import *\n",
    "from database_helpers.create_tables import *\n",
    "from database_helpers.write2tables import *\n",
    "\n",
    "# register pickle type to retrieve binary data from database\n",
    "psycopg2.extensions.register_type(psycopg2.extensions.new_type(psycopg2.BINARY.values, 'BINARY-PICKLE', cast_pickle))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postgresql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try a first handshake with the database via the function `test_connect()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to the PostgreSQL database...\n",
      "Grrr... no database connection could be established.\n"
     ]
    }
   ],
   "source": [
    "db_status = test_connect()\n",
    "if db_status == False:\n",
    "    print('Grrr... no database connection could be established.')\n",
    "else:\n",
    "    print('Yippiyeah! Database connection is established!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query database with pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every retrieval of data consists of **two steps**: Creation of sql query command, and execution of that command through pandas to retrieve a dataframe. To keep the data size small it is important to filter the data to be retrieved, but let's skip that for now (see below). Once loaded in a pandas dataframe, operations are run locally and additional filtering / manipulations can be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the parameters to connect to data_1 \n",
    "params = config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "fe_sendauth: no password supplied\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-5c42ff21b3d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu'sql = \"SELECT * FROM meta_tb\" # sql command \\nsql_db_pd = pd.read_sql_query(sql, psycopg2.connect(**params), index_col=None) # execute query and read into pandas dataframe'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mc:\\users\\horsto\\appdata\\local\\continuum\\miniconda2\\envs\\hotte\\lib\\site-packages\\IPython\\core\\interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2113\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2114\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32mc:\\users\\horsto\\appdata\\local\\continuum\\miniconda2\\envs\\hotte\\lib\\site-packages\\IPython\\core\\magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\users\\horsto\\appdata\\local\\continuum\\miniconda2\\envs\\hotte\\lib\\site-packages\\IPython\\core\\magics\\execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1178\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[1;32mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mc:\\users\\horsto\\appdata\\local\\continuum\\miniconda2\\envs\\hotte\\lib\\site-packages\\psycopg2\\__init__.pyc\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(dsn, database, user, password, host, port, connection_factory, cursor_factory, async, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m                 for (k, v) in items])\n\u001b[1;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m     \u001b[0mconn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_connect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdsn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconnection_factory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconnection_factory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0masync\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0masync\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcursor_factory\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcursor_factory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcursor_factory\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOperationalError\u001b[0m: fe_sendauth: no password supplied\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "sql = \"SELECT * FROM meta_tb\" # sql command \n",
    "sql_db_pd = pd.read_sql_query(sql, psycopg2.connect(**params), index_col=None) # execute query and read into pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above loads the parameters (`params`) to connect to the database data_1. This only has to be done once per notebook. \n",
    "Then a sql command is created (`sql`) and it passed into a pandas `read_sql_query` command to retrieve a pandas dataframe object for that query. \n",
    "\n",
    "Let's look at that sql query:\n",
    "\n",
    "`SELECT * FROM meta_tb` \n",
    "- SELECT - we want to read data, \n",
    "- we don't filter any columns (*) -- give me all the columns that are available \n",
    "- FROM meta_tb - which table to we want to read from\n",
    "\n",
    "Let's see what we got. A convenient way to look at a pandas dataframe object is just to call it and pass a .head() or .tail() to it, which will show a snippet of the start or the end. An optional number within brackets defines how many rows you want to show. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sql_db_pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have retrieved the complete meta_tb table and can inspect the output as dataframe object. Let's make both the sql query as well as the pandas function a bit more sophisticated:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sql = \"SELECT * FROM meta_tb WHERE session_ts > '1/6/2016' AND session_ts < '25/04/2017'\"\n",
    "sql_db_pd = pd.read_sql_query(sql, psycopg2.connect(**params), index_col=None,parse_dates=['session_ts','analysis_ts'])\n",
    "sql_db_pd.sort_values(by='analysis_ts', ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing that statement should have produced a view of the top 3 meta table entries sorted by analysis timestamp in descending order. \n",
    "\n",
    "What we integrated was a **filter** option (via `WHERE` in the sql query, filtering for a date range in the session timestamp column) and an additional parameter (`parse_dates`) in the pandas retrieval function, such that dates are parsed correctly. We also told pandas to sort the output for the analysis timestamp (`ascending=False`).\n",
    "\n",
    "So in practice filtering can be done on the level of the database interaction within the sql command AND later on in the dataframe. The first filtering step is the most important one because we want to keep the dataset that is transferred via the local network small. But quering with a complex statement can put significant strain on the database server - so there is a trade off ... \n",
    "\n",
    "One more example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sql = \"SELECT * FROM clusters_tb\"\n",
    "sql_db_pd = pd.read_sql_query(sql, psycopg2.connect(**params), index_col=None,parse_dates=['session_ts','analysis_ts'])\n",
    "sql_db_pd.sort_values(by='analysis_ts', ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the next notebook for data retrieval examples."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
